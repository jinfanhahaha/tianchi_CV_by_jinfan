{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"collapsed":true},"cell_type":"code","source":"! pip install albumentations\n! pip install pretrainedmodels","execution_count":1,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: albumentations in /opt/conda/lib/python3.7/site-packages (0.4.6)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from albumentations) (1.4.1)\nRequirement already satisfied: opencv-python>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from albumentations) (4.4.0.44)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from albumentations) (5.3.1)\nRequirement already satisfied: numpy>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from albumentations) (1.18.5)\nRequirement already satisfied: imgaug>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from albumentations) (0.4.0)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (3.2.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (1.14.0)\nRequirement already satisfied: scikit-image>=0.14.2 in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (0.16.2)\nRequirement already satisfied: imageio in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (2.8.0)\nRequirement already satisfied: Shapely in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (1.7.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (8.0.0)\nRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.8.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (0.10.0)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.4.7)\nRequirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations) (2.4)\nRequirement already satisfied: PyWavelets>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations) (1.1.1)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug>=0.4.0->albumentations) (4.4.2)\nCollecting pretrainedmodels\n  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n\u001b[K     |████████████████████████████████| 58 kB 434 kB/s eta 0:00:011\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels) (1.6.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels) (0.7.0)\nRequirement already satisfied: munch in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels) (2.5.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels) (4.45.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->pretrainedmodels) (0.18.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch->pretrainedmodels) (1.18.5)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision->pretrainedmodels) (8.0.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from munch->pretrainedmodels) (1.14.0)\nBuilding wheels for collected packages: pretrainedmodels\n  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60962 sha256=464bcad82e1e628b84bb3769d64fe32d1b332c4c0b5ce46632148af719fcb482\n  Stored in directory: /root/.cache/pip/wheels/ed/27/e8/9543d42de2740d3544db96aefef63bda3f2c1761b3334f4873\nSuccessfully built pretrainedmodels\nInstalling collected packages: pretrainedmodels\nSuccessfully installed pretrainedmodels-0.7.4\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## 一、导入库"},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\nfrom torchvision import models\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom collections import Counter\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import FancyPCA\nimport albumentations as A\nimport pretrainedmodels\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nimport torch.utils.data as data\nimport cv2\nimport os\nimport random\nimport pandas as pd\nimport json\n\n# 设置随机种子\nseed = 2020\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)            # 为CPU设置随机种子\ntorch.cuda.manual_seed(seed)       # 为当前GPU设置随机种子\ntorch.cuda.manual_seed_all(seed)   # 为所有GPU设置随机种子\nos.environ['PYTHONHASHSEED'] = str(seed) # 为了禁止hash随机化，使得实验可复现。","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 二、获取路径以及类别,并划分训练验证集"},{"metadata":{"trusted":true},"cell_type":"code","source":"DIR_INPUT = '/kaggle/input/plant-pathology-2020-fgvc7/images/'\ndata_df = pd.read_csv('../input/plant-pathology-2020-fgvc7/train.csv')\ntest_df = pd.read_csv('../input/plant-pathology-2020-fgvc7/test.csv')\ndata_paths = [DIR_INPUT + id for id in data_df['image_id']]\ntest_paths = [DIR_INPUT + id for id in test_df['image_id']]\ndata_labels = []\nfor i in range(len(data_df)):\n    label = data_df.loc[i, ['healthy', 'multiple_diseases', 'rust', 'scab']].values\n    data_labels.append(int(np.argwhere(label==1)))\n\n# 打散\ndata_paths = np.array(data_paths)\ndata_labels = np.array(data_labels)\np = np.random.permutation(len(data_labels))\ndata_paths = data_paths[p]\ndata_labels = data_labels[p]\n\n# 划分训练验证集\nsplit_k = len(data_paths) // 8\ntrain_paths = data_paths[split_k:]\ntrain_labels = data_labels[split_k:]\nval_paths = data_paths[:split_k]\nval_labels = data_labels[:split_k]\n","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. ## 三、制作data loader类"},{"metadata":{"trusted":true},"cell_type":"code","source":"# data loader\nclass ImageDataset(data.Dataset):\n    def __init__(self, paths, labels, transform=None):\n        self.paths = paths\n        self.labels = labels\n        self.transform = transform\n\n    def __getitem__(self, index):\n        img_path = self.paths[index]\n        label = self.labels[index]\n        image = cv2.imread(img_path + \".jpg\")\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        if self.transform is not None:\n            res = self.transform(image=image)\n            image = res['image']\n        return image, int(label)\n\n    def __len__(self):\n        return len(self.paths)\n\n# test loader\nclass TestDataset(data.Dataset):\n    def __init__(self, paths, transform=None):\n        self.paths = paths\n        self.transform = transform\n\n    def __getitem__(self, index):\n        img_path = self.paths[index]\n        image = cv2.imread(img_path + \".jpg\")\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        if self.transform is not None:\n            res = self.transform(image=image)\n            image = res['image']\n        return image\n\n    def __len__(self):\n        return len(self.paths)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_transform = A.Compose([\n            A.Resize(height=512, width=512),\n            A.OneOf([\n                A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),\n                A.RandomBrightness(limit=0.1, p=0.5),\n            ], p=1),\n            A.GaussNoise(),\n            A.HorizontalFlip(p=0.5),\n            A.RandomRotate90(p=0.5),\n            A.ShiftScaleRotate(rotate_limit=1, p=0.5),\n            FancyPCA(alpha=0.1, p=0.5),\n            # blur\n            A.OneOf([\n                A.MotionBlur(blur_limit=3), A.MedianBlur(blur_limit=3), A.GaussianBlur(blur_limit=3),\n            ], p=0.5),\n            # Pixels\n            A.OneOf([\n                A.IAAEmboss(p=0.5),\n                A.IAASharpen(p=0.5),\n            ], p=1),\n            # Affine\n            A.OneOf([\n                A.ElasticTransform(p=0.5),\n                A.IAAPiecewiseAffine(p=0.5),\n            ], p=1),\n            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0),\n            ToTensorV2(p=1.0),\n        ])\n\nvalid_transform = A.Compose([\n            A.Resize(height=512, width=512),\n            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0),\n            ToTensorV2(p=1.0),\n        ])\n\n\ntest_transform = A.Compose([\n            A.Resize(height=512, width=512),\n            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0),\n            ToTensorV2(p=1.0),\n        ])\n\ntrainset = ImageDataset(train_paths, train_labels, train_transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=16, shuffle=True, num_workers=4)\nvalidset = ImageDataset(val_paths, val_labels, valid_transform)\nvalidloader = torch.utils.data.DataLoader(validset, batch_size=16, shuffle=False, num_workers=4)\ntestset = TestDataset(test_paths, test_transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=16, shuffle=False, num_workers=4)","execution_count":19,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 四、构建模型"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self, alexnet):\n        super(Net, self).__init__()\n        self.backbone = alexnet._features\n        self.fc = nn.Linear(1024, 4)\n\n    def forward(self, x):\n        batch_size, C, H, W = x.shape\n        x = self.backbone(x)\n        x = F.adaptive_avg_pool2d(x, 2).reshape(batch_size, -1)\n        x = self.fc(x)\n        return x","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 根据环境自动选择是否使用GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n# 使用Alexnet网络进行图像分类\nalexnet = pretrainedmodels.__dict__['alexnet'](num_classes=1000, pretrained='imagenet')\n# 将原始alexnet送入自定义的类中\nalexnet = Net(alexnet)\n\nalexnet.to(device)\n# 打印alexnet网络结构\nprint(alexnet)","execution_count":7,"outputs":[{"output_type":"stream","text":"Downloading: \"https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-4df8aa71.pth\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=244418560.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e5bfa6ae1644e6da0b1b1df92016a4f"}},"metadata":{}},{"output_type":"stream","text":"\nNet(\n  (backbone): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (fc): Linear(in_features=1024, out_features=4, bias=True)\n)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## 五、配置训练参数"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 使用交叉熵损失进行训练\ncriterion = nn.CrossEntropyLoss()\n# 使用随机梯度下降法进行优化模型，并且设置学习率为0.001\noptimizer = optim.SGD(alexnet.parameters(), lr=0.001, momentum=0.9)\n# 训练2个epoch\nEPOCH = 2","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 六、训练"},{"metadata":{"trusted":true},"cell_type":"code","source":"for epoch in range(EPOCH):\n    running_loss = 0\n    train_correct = 0\n    train_total = 0\n    for i, data in enumerate(trainloader):\n        images, labels = data[0].to(device), data[1].to(device, dtype=torch.int64)\n        optimizer.zero_grad()\n        outputs = alexnet(images)\n        _, predicted = torch.max(outputs.data, 1)\n        train_total += labels.size(0)\n        train_correct += (predicted == labels).sum().item()\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        if i % 20 == 19:\n            print('[%d, %5d] loss: %.3f' % (epoch+1, i+1, running_loss/20))\n            running_loss = 0.0\n    train_accuracy = train_correct / train_total\n    print('train dataset accuracy %.4f' % train_accuracy)\n    \n    test_correct = 0\n    test_total = 0\n    res = []\n    with torch.no_grad():\n        for data in validloader:\n            images, labels = data[0].to(device), data[1].to(device, dtype=torch.int64)\n            outputs = alexnet(images)\n            _, predicted = torch.max(outputs.data, 1)\n            for p in predicted:\n                res.append(int(p))\n\n    print(\"Val Accuracy: \", np.sum(np.array(res)==np.array(val_labels)) / len(val_labels))","execution_count":9,"outputs":[{"output_type":"stream","text":"[1,    20] loss: 1.213\n[1,    40] loss: 1.112\n[1,    60] loss: 0.890\n[1,    80] loss: 0.608\n[1,   100] loss: 0.635\ntrain dataset accuracy 0.6305\nVal Accuracy:  0.8325991189427313\n[2,    20] loss: 0.592\n[2,    40] loss: 0.535\n[2,    60] loss: 0.519\n[2,    80] loss: 0.668\n[2,   100] loss: 0.381\ntrain dataset accuracy 0.8156\nVal Accuracy:  0.8502202643171806\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## 七、生成提交文件"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = None\nsubmission_df = pd.read_csv('../input/plant-pathology-2020-fgvc7/sample_submission.csv')\nwith torch.no_grad():\n    for data in testloader:\n        images = data.to(device)\n        outputs = alexnet(images)\n\n        if test_preds is None:\n            test_preds = outputs.data.cpu()\n        else:\n            test_preds = torch.cat((test_preds, outputs.data.cpu()), dim=0)\n\nsubmission_df[['healthy', 'multiple_diseases', 'rust', 'scab']] = torch.softmax(test_preds, dim=1)\nsubmission_df.to_csv('submission.csv', index=False)","execution_count":21,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"not enough values to unpack (expected 4, got 3)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-b84bc82b0810>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malexnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtest_preds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-994a9c1daa73>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madaptive_avg_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}